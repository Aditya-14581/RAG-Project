{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary packages\n",
    "# !pip install llama-index llama-index-llms-huggingface llama-index-embeddings-huggingface transformers accelerate bitsandbytes llama-index-readers-web\n",
    "\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_index.llms.huggingface import HuggingFaceLLM\n",
    "from llama_index.core import Settings\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "# Step 1: Scrape “Luke Skywalker” wiki page\n",
    "# Define the URL of the Wikipedia page\n",
    "URL = \"https://en.wikipedia.org/wiki/Luke_Skywalker\"\n",
    "\n",
    "# Load the BeautifulSoupWebReader to scrape the web page\n",
    "from llama_index.core import download_loader\n",
    "BeautifulSoupWebReader = download_loader(\"BeautifulSoupWebReader\")\n",
    "loader = BeautifulSoupWebReader()\n",
    "documents = loader.load_data(urls=[URL])\n",
    "\n",
    "# Step 2: Chunk it, store it in any vector database like Faiss.\n",
    "# Configure the HuggingFaceLLM with quantization for efficiency\n",
    "import torch\n",
    "from transformers import BitsAndBytesConfig\n",
    "from llama_index.llms.huggingface import HuggingFaceLLM\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "# Initialize the LLM with a specific model\n",
    "Settings.llm = HuggingFaceLLM(\n",
    "    model_name=\"berkeley-nest/Starling-LM-7B-alpha\",\n",
    "    tokenizer_name=\"berkeley-nest/Starling-LM-7B-alpha\",\n",
    "    context_window=3900,\n",
    "    max_new_tokens=256,\n",
    "    model_kwargs={\"quantization_config\": quantization_config},\n",
    "    generate_kwargs={\"temperature\": 0.8, \"do_sample\": True},\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "# Step 3: Add any LLM API calling functionality like openai/gemini/claude/groq/deep infra etc.\n",
    "from llama_index.core import Settings\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "# Settings.llm = llm\n",
    "Settings.embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "\n",
    "# Step 4: Create a VectorStoreIndex for chunking and storing the data\n",
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "Settings.chunk_size = 512\n",
    "Settings.chunk_overlap = 50\n",
    "\n",
    "# Create the vector index from the documents\n",
    "vector_index = VectorStoreIndex.from_documents(\n",
    "    documents,\n",
    ")\n",
    "\n",
    "# Persist the vector index for future use\n",
    "# vector_index.storage_context.persist(persist_dir='/content')\n",
    "\n",
    "# Step 5: From question, retrieve top 3 relevant chunks\n",
    "# Step 6: Pass actual questions with retrieved chunks to LLM.\n",
    "# Create a query engine to find the top 3 relevant chunks\n",
    "query_engine = vector_index.as_query_engine(similarity_top_k=3)\n",
    "\n",
    "# Example query to the engine, You can type your query here\n",
    "\n",
    "text=input(\"Input your query here about Luke Skywalker: \")\n",
    "\n",
    "response = query_engine.query(text)\n",
    "print(response)\n",
    "# Step 7: Give an answer.\n",
    "# Display the answer in markdown format\n",
    "# display(Markdown(f\"<b>{response}</b>\"))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
